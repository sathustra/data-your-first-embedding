{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your first embedding\n",
    "\n",
    "## Exercise objectives:\n",
    "- Run your first RNN for NLP\n",
    "- Get a first taste of what an embedding is\n",
    "\n",
    "<hr>\n",
    "\n",
    "Words are not something you can easily feed to a Neural Network. For this reason, we have to convert them to something more meaningful. \n",
    "\n",
    "And this is exactly what _Embeddings_ are for! They map any word onto a vectorial representation (this a fancy way to represent each word with a vector ;) ). For instance, the word `dog` can be represented by the vector $(w_1, w_2, ..., w_n)$ in the embedding space, and we will learn the weights $(w_k)_k$.\n",
    "\n",
    "So let's just do it.\n",
    "\n",
    "\n",
    "# The data\n",
    "\n",
    "\n",
    "❓ **Question** ❓ Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "⚠️ **Warning** ⚠️ The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that too many sentences will make your compute slow down, or even freeze - your RAM can overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n",
    "\n",
    "⚠️ **DISCLAIMER** ⚠️ **No need to play _who has the biggest_ (RAM) !** The idea is to get to run your models quickly to prototype. Even in real life, it is recommended that you start with a subset of your data to loop and debug quickly. So increase the number only if you are into getting the best accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Just run this cell to load the data ###\n",
    "###########################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have loaded the data, let's check it out!\n",
    "\n",
    "❓ **Question** ❓ You can play with the data here. In particular, `X_train` and `X_test` are lists of sentences. Let's print some of them, with their respective label stored in `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_sentences\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_sentences' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/11/qmf8fycd0_d7cflbxhz7fnfw0000gn/T/ipykernel_34599/2666856182.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(X_train).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "np.array(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'was',\n",
       " 'an',\n",
       " 'absolutely',\n",
       " 'terrible',\n",
       " 'movie',\n",
       " \"don't\",\n",
       " 'be',\n",
       " 'lured',\n",
       " 'in',\n",
       " 'by',\n",
       " 'christopher',\n",
       " 'walken',\n",
       " 'or',\n",
       " 'michael',\n",
       " 'ironside',\n",
       " 'both',\n",
       " 'are',\n",
       " 'great',\n",
       " 'actors',\n",
       " 'but',\n",
       " 'this',\n",
       " 'must',\n",
       " 'simply',\n",
       " 'be',\n",
       " 'their',\n",
       " 'worst',\n",
       " 'role',\n",
       " 'in',\n",
       " 'history',\n",
       " 'even',\n",
       " 'their',\n",
       " 'great',\n",
       " 'acting',\n",
       " 'could',\n",
       " 'not',\n",
       " 'redeem',\n",
       " 'this',\n",
       " \"movie's\",\n",
       " 'ridiculous',\n",
       " 'storyline',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'an',\n",
       " 'early',\n",
       " 'nineties',\n",
       " 'us',\n",
       " 'propaganda',\n",
       " 'piece',\n",
       " 'the',\n",
       " 'most',\n",
       " 'pathetic',\n",
       " 'scenes',\n",
       " 'were',\n",
       " 'those',\n",
       " 'when',\n",
       " 'the',\n",
       " 'columbian',\n",
       " 'rebels',\n",
       " 'were',\n",
       " 'making',\n",
       " 'their',\n",
       " 'cases',\n",
       " 'for',\n",
       " 'revolutions',\n",
       " 'maria',\n",
       " 'conchita',\n",
       " 'alonso',\n",
       " 'appeared',\n",
       " 'phony',\n",
       " 'and',\n",
       " 'her',\n",
       " 'pseudo',\n",
       " 'love',\n",
       " 'affair',\n",
       " 'with',\n",
       " 'walken',\n",
       " 'was',\n",
       " 'nothing',\n",
       " 'but',\n",
       " 'a',\n",
       " 'pathetic',\n",
       " 'emotional',\n",
       " 'plug',\n",
       " 'in',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'was',\n",
       " 'devoid',\n",
       " 'of',\n",
       " 'any',\n",
       " 'real',\n",
       " 'meaning',\n",
       " 'i',\n",
       " 'am',\n",
       " 'disappointed',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'this',\n",
       " 'ruining',\n",
       " \"actor's\",\n",
       " 'like',\n",
       " 'christopher',\n",
       " \"walken's\",\n",
       " 'good',\n",
       " 'name',\n",
       " 'i',\n",
       " 'could',\n",
       " 'barely',\n",
       " 'sit',\n",
       " 'through',\n",
       " 'it']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LABELS**, the task is a binary classification problem:\n",
    "- label 0 corresponds to a negative movie review\n",
    "- label 1 corresponds to a positive movie review\n",
    "\n",
    "**INPUTS**: The data has been partially cleaned! So you don't have to worry about it in this exercise. But don't forget this step in real-life challenges. \n",
    "\n",
    "Remember that words are not computer-compatible materials? You have to tokenize them!\n",
    "\n",
    "❓ **Question** ❓ Run the following cell to tokenize your sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# This initializes a Keras utilities that does all the tokenization for you\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# The tokenization learns a dictionary that maps a token (integer) to each word\n",
    "# It can be done only on the train set - we are not supposed to know the test set!\n",
    "# This tokenization also lowercases your words, apply some filters, and so on - you can check the doc if you want\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Print some of the tokenized sentences to be sure you got what you expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/11/qmf8fycd0_d7cflbxhz7fnfw0000gn/T/ipykernel_34599/1151753299.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(X_train_token).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train_token).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'was',\n",
       " 'an',\n",
       " 'absolutely',\n",
       " 'terrible',\n",
       " 'movie',\n",
       " \"don't\",\n",
       " 'be',\n",
       " 'lured',\n",
       " 'in',\n",
       " 'by',\n",
       " 'christopher',\n",
       " 'walken',\n",
       " 'or',\n",
       " 'michael',\n",
       " 'ironside',\n",
       " 'both',\n",
       " 'are',\n",
       " 'great',\n",
       " 'actors',\n",
       " 'but',\n",
       " 'this',\n",
       " 'must',\n",
       " 'simply',\n",
       " 'be',\n",
       " 'their',\n",
       " 'worst',\n",
       " 'role',\n",
       " 'in',\n",
       " 'history',\n",
       " 'even',\n",
       " 'their',\n",
       " 'great',\n",
       " 'acting',\n",
       " 'could',\n",
       " 'not',\n",
       " 'redeem',\n",
       " 'this',\n",
       " \"movie's\",\n",
       " 'ridiculous',\n",
       " 'storyline',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'an',\n",
       " 'early',\n",
       " 'nineties',\n",
       " 'us',\n",
       " 'propaganda',\n",
       " 'piece',\n",
       " 'the',\n",
       " 'most',\n",
       " 'pathetic',\n",
       " 'scenes',\n",
       " 'were',\n",
       " 'those',\n",
       " 'when',\n",
       " 'the',\n",
       " 'columbian',\n",
       " 'rebels',\n",
       " 'were',\n",
       " 'making',\n",
       " 'their',\n",
       " 'cases',\n",
       " 'for',\n",
       " 'revolutions',\n",
       " 'maria',\n",
       " 'conchita',\n",
       " 'alonso',\n",
       " 'appeared',\n",
       " 'phony',\n",
       " 'and',\n",
       " 'her',\n",
       " 'pseudo',\n",
       " 'love',\n",
       " 'affair',\n",
       " 'with',\n",
       " 'walken',\n",
       " 'was',\n",
       " 'nothing',\n",
       " 'but',\n",
       " 'a',\n",
       " 'pathetic',\n",
       " 'emotional',\n",
       " 'plug',\n",
       " 'in',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'was',\n",
       " 'devoid',\n",
       " 'of',\n",
       " 'any',\n",
       " 'real',\n",
       " 'meaning',\n",
       " 'i',\n",
       " 'am',\n",
       " 'disappointed',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'this',\n",
       " 'ruining',\n",
       " \"actor's\",\n",
       " 'like',\n",
       " 'christopher',\n",
       " \"walken's\",\n",
       " 'good',\n",
       " 'name',\n",
       " 'i',\n",
       " 'could',\n",
       " 'barely',\n",
       " 'sit',\n",
       " 'through',\n",
       " 'it']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'and': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'is': 6,\n",
       " 'br': 7,\n",
       " 'in': 8,\n",
       " 'i': 9,\n",
       " 'it': 10,\n",
       " 'this': 11,\n",
       " 'that': 12,\n",
       " 'was': 13,\n",
       " 'as': 14,\n",
       " 'for': 15,\n",
       " 'with': 16,\n",
       " 'but': 17,\n",
       " 'movie': 18,\n",
       " 'film': 19,\n",
       " 'on': 20,\n",
       " 'not': 21,\n",
       " 'you': 22,\n",
       " 'his': 23,\n",
       " 'are': 24,\n",
       " 'have': 25,\n",
       " 'one': 26,\n",
       " 'be': 27,\n",
       " 'he': 28,\n",
       " 'all': 29,\n",
       " 'at': 30,\n",
       " 'by': 31,\n",
       " 'they': 32,\n",
       " 'an': 33,\n",
       " 'so': 34,\n",
       " 'like': 35,\n",
       " 'who': 36,\n",
       " 'from': 37,\n",
       " 'her': 38,\n",
       " 'or': 39,\n",
       " 'just': 40,\n",
       " 'if': 41,\n",
       " 'out': 42,\n",
       " 'about': 43,\n",
       " \"it's\": 44,\n",
       " 'has': 45,\n",
       " 'what': 46,\n",
       " 'some': 47,\n",
       " 'there': 48,\n",
       " 'good': 49,\n",
       " 'more': 50,\n",
       " 'when': 51,\n",
       " 'very': 52,\n",
       " 'no': 53,\n",
       " 'up': 54,\n",
       " 'she': 55,\n",
       " 'my': 56,\n",
       " 'time': 57,\n",
       " 'even': 58,\n",
       " 'which': 59,\n",
       " 'would': 60,\n",
       " 'really': 61,\n",
       " 'only': 62,\n",
       " 'had': 63,\n",
       " 'story': 64,\n",
       " 'me': 65,\n",
       " 'see': 66,\n",
       " 'can': 67,\n",
       " 'their': 68,\n",
       " 'were': 69,\n",
       " 'well': 70,\n",
       " 'than': 71,\n",
       " 'much': 72,\n",
       " 'get': 73,\n",
       " 'do': 74,\n",
       " 'great': 75,\n",
       " 'been': 76,\n",
       " 'we': 77,\n",
       " 'first': 78,\n",
       " 'bad': 79,\n",
       " 'because': 80,\n",
       " 'into': 81,\n",
       " 'other': 82,\n",
       " 'will': 83,\n",
       " 'how': 84,\n",
       " 'also': 85,\n",
       " 'most': 86,\n",
       " 'then': 87,\n",
       " 'too': 88,\n",
       " 'him': 89,\n",
       " 'made': 90,\n",
       " \"don't\": 91,\n",
       " 'people': 92,\n",
       " 'way': 93,\n",
       " 'them': 94,\n",
       " 'make': 95,\n",
       " 'its': 96,\n",
       " 'movies': 97,\n",
       " 'could': 98,\n",
       " 'any': 99,\n",
       " 'films': 100,\n",
       " 'think': 101,\n",
       " 'watch': 102,\n",
       " 'plot': 103,\n",
       " 'after': 104,\n",
       " 'two': 105,\n",
       " 'best': 106,\n",
       " 'many': 107,\n",
       " 'acting': 108,\n",
       " 'life': 109,\n",
       " 'character': 110,\n",
       " 'characters': 111,\n",
       " 'never': 112,\n",
       " 'seen': 113,\n",
       " 'where': 114,\n",
       " 'being': 115,\n",
       " 'did': 116,\n",
       " 'over': 117,\n",
       " 'little': 118,\n",
       " 'these': 119,\n",
       " 'love': 120,\n",
       " 'end': 121,\n",
       " 'man': 122,\n",
       " 'show': 123,\n",
       " 'better': 124,\n",
       " 'off': 125,\n",
       " 'ever': 126,\n",
       " 'know': 127,\n",
       " 'still': 128,\n",
       " 'does': 129,\n",
       " 'say': 130,\n",
       " 'while': 131,\n",
       " 'your': 132,\n",
       " 'scene': 133,\n",
       " 'go': 134,\n",
       " 'why': 135,\n",
       " 'should': 136,\n",
       " 'through': 137,\n",
       " 'such': 138,\n",
       " 'here': 139,\n",
       " 'scenes': 140,\n",
       " 'something': 141,\n",
       " 'real': 142,\n",
       " 'back': 143,\n",
       " 'now': 144,\n",
       " 'thing': 145,\n",
       " \"i'm\": 146,\n",
       " \"doesn't\": 147,\n",
       " 'find': 148,\n",
       " 'before': 149,\n",
       " \"didn't\": 150,\n",
       " 'though': 151,\n",
       " 'makes': 152,\n",
       " 'old': 153,\n",
       " 'again': 154,\n",
       " 'watching': 155,\n",
       " 'funny': 156,\n",
       " 'new': 157,\n",
       " 'those': 158,\n",
       " 'every': 159,\n",
       " 'director': 160,\n",
       " 'actors': 161,\n",
       " 'years': 162,\n",
       " 'going': 163,\n",
       " 'nothing': 164,\n",
       " 'actually': 165,\n",
       " 'another': 166,\n",
       " 'look': 167,\n",
       " 'same': 168,\n",
       " 'work': 169,\n",
       " 'part': 170,\n",
       " '10': 171,\n",
       " 'pretty': 172,\n",
       " 'want': 173,\n",
       " 'quite': 174,\n",
       " 'few': 175,\n",
       " 'young': 176,\n",
       " 'thought': 177,\n",
       " 'down': 178,\n",
       " 'bit': 179,\n",
       " 'horror': 180,\n",
       " 'own': 181,\n",
       " 'cast': 182,\n",
       " \"can't\": 183,\n",
       " 'give': 184,\n",
       " 'around': 185,\n",
       " 'fact': 186,\n",
       " 'lot': 187,\n",
       " \"that's\": 188,\n",
       " 'world': 189,\n",
       " 'things': 190,\n",
       " 'enough': 191,\n",
       " 'point': 192,\n",
       " 'series': 193,\n",
       " 'both': 194,\n",
       " 'may': 195,\n",
       " 'done': 196,\n",
       " 'long': 197,\n",
       " 'seems': 198,\n",
       " 'got': 199,\n",
       " 'however': 200,\n",
       " 'gets': 201,\n",
       " 'take': 202,\n",
       " 'role': 203,\n",
       " 'us': 204,\n",
       " 'always': 205,\n",
       " 'original': 206,\n",
       " 'between': 207,\n",
       " 'big': 208,\n",
       " 'right': 209,\n",
       " 'saw': 210,\n",
       " 'music': 211,\n",
       " 'times': 212,\n",
       " 'without': 213,\n",
       " \"i've\": 214,\n",
       " \"isn't\": 215,\n",
       " 'family': 216,\n",
       " 'far': 217,\n",
       " 'whole': 218,\n",
       " 'might': 219,\n",
       " 'minutes': 220,\n",
       " 'am': 221,\n",
       " 'come': 222,\n",
       " 'least': 223,\n",
       " \"there's\": 224,\n",
       " '2': 225,\n",
       " 'action': 226,\n",
       " 'script': 227,\n",
       " 'must': 228,\n",
       " 'tv': 229,\n",
       " 'since': 230,\n",
       " 'kind': 231,\n",
       " 'away': 232,\n",
       " 'interesting': 233,\n",
       " 'almost': 234,\n",
       " 'guy': 235,\n",
       " 'fun': 236,\n",
       " 'performance': 237,\n",
       " 'rather': 238,\n",
       " 'especially': 239,\n",
       " 'anything': 240,\n",
       " 'hard': 241,\n",
       " 'last': 242,\n",
       " 'actor': 243,\n",
       " 'sure': 244,\n",
       " 'believe': 245,\n",
       " 'three': 246,\n",
       " 'girl': 247,\n",
       " 'comedy': 248,\n",
       " \"he's\": 249,\n",
       " 'ending': 250,\n",
       " 'making': 251,\n",
       " 'worst': 252,\n",
       " 'looking': 253,\n",
       " 'anyone': 254,\n",
       " 'woman': 255,\n",
       " 'screen': 256,\n",
       " 'day': 257,\n",
       " 'trying': 258,\n",
       " 'maybe': 259,\n",
       " 'found': 260,\n",
       " 'course': 261,\n",
       " 'watched': 262,\n",
       " 'each': 263,\n",
       " 'having': 264,\n",
       " 'set': 265,\n",
       " 'probably': 266,\n",
       " 'yet': 267,\n",
       " 'although': 268,\n",
       " 'book': 269,\n",
       " 'comes': 270,\n",
       " 'feel': 271,\n",
       " 'money': 272,\n",
       " 'everything': 273,\n",
       " 'played': 274,\n",
       " \"wasn't\": 275,\n",
       " 'night': 276,\n",
       " 'true': 277,\n",
       " 'goes': 278,\n",
       " 'instead': 279,\n",
       " 'put': 280,\n",
       " 'sense': 281,\n",
       " 'reason': 282,\n",
       " 'place': 283,\n",
       " 'high': 284,\n",
       " 'dvd': 285,\n",
       " 'audience': 286,\n",
       " 'main': 287,\n",
       " 'looks': 288,\n",
       " 'later': 289,\n",
       " '1': 290,\n",
       " 'different': 291,\n",
       " 'job': 292,\n",
       " 'second': 293,\n",
       " 'once': 294,\n",
       " 'shows': 295,\n",
       " 'during': 296,\n",
       " 'american': 297,\n",
       " 'else': 298,\n",
       " 'together': 299,\n",
       " 'takes': 300,\n",
       " 'worth': 301,\n",
       " 'someone': 302,\n",
       " 'plays': 303,\n",
       " 'fan': 304,\n",
       " 'wife': 305,\n",
       " 'our': 306,\n",
       " 'said': 307,\n",
       " 'play': 308,\n",
       " 'star': 309,\n",
       " 'year': 310,\n",
       " 'read': 311,\n",
       " 'effects': 312,\n",
       " 'version': 313,\n",
       " 'home': 314,\n",
       " 'special': 315,\n",
       " 'half': 316,\n",
       " 'black': 317,\n",
       " 'idea': 318,\n",
       " 'boring': 319,\n",
       " 'seeing': 320,\n",
       " 'given': 321,\n",
       " 'help': 322,\n",
       " '3': 323,\n",
       " 'less': 324,\n",
       " 'war': 325,\n",
       " 'beautiful': 326,\n",
       " 'house': 327,\n",
       " 'seem': 328,\n",
       " 'himself': 329,\n",
       " 'death': 330,\n",
       " \"you're\": 331,\n",
       " 'production': 332,\n",
       " 'left': 333,\n",
       " 'mind': 334,\n",
       " 'excellent': 335,\n",
       " 'simply': 336,\n",
       " 'john': 337,\n",
       " 'top': 338,\n",
       " 'either': 339,\n",
       " 'short': 340,\n",
       " 'camera': 341,\n",
       " 'hollywood': 342,\n",
       " 'stupid': 343,\n",
       " 'poor': 344,\n",
       " 'shot': 345,\n",
       " 'couple': 346,\n",
       " 'father': 347,\n",
       " 'used': 348,\n",
       " 'perfect': 349,\n",
       " 'line': 350,\n",
       " 'dead': 351,\n",
       " 'until': 352,\n",
       " 'nice': 353,\n",
       " \"couldn't\": 354,\n",
       " 'kids': 355,\n",
       " 'understand': 356,\n",
       " 'try': 357,\n",
       " 'enjoy': 358,\n",
       " 'classic': 359,\n",
       " 'completely': 360,\n",
       " 'mean': 361,\n",
       " 'along': 362,\n",
       " 'wrong': 363,\n",
       " 'awful': 364,\n",
       " 'rest': 365,\n",
       " 'perhaps': 366,\n",
       " 'everyone': 367,\n",
       " 'start': 368,\n",
       " 'need': 369,\n",
       " 'performances': 370,\n",
       " 'truly': 371,\n",
       " 'men': 372,\n",
       " 'boy': 373,\n",
       " \"i'd\": 374,\n",
       " 'name': 375,\n",
       " 'finally': 376,\n",
       " 'liked': 377,\n",
       " 'budget': 378,\n",
       " 'keep': 379,\n",
       " 'small': 380,\n",
       " 'gives': 381,\n",
       " 'head': 382,\n",
       " 'felt': 383,\n",
       " 'wonderful': 384,\n",
       " 'full': 385,\n",
       " 'face': 386,\n",
       " 'style': 387,\n",
       " '\\x96': 388,\n",
       " 'early': 389,\n",
       " 'mr': 390,\n",
       " 'stars': 391,\n",
       " 'let': 392,\n",
       " 'low': 393,\n",
       " 'friends': 394,\n",
       " 'tell': 395,\n",
       " 'drama': 396,\n",
       " 'hope': 397,\n",
       " 'written': 398,\n",
       " 'school': 399,\n",
       " 'dialogue': 400,\n",
       " 'recommend': 401,\n",
       " 'terrible': 402,\n",
       " 'case': 403,\n",
       " 'worse': 404,\n",
       " 'based': 405,\n",
       " 'sort': 406,\n",
       " 'moments': 407,\n",
       " 'run': 408,\n",
       " 'playing': 409,\n",
       " 'next': 410,\n",
       " 'video': 411,\n",
       " 'supposed': 412,\n",
       " 'mother': 413,\n",
       " 'often': 414,\n",
       " 'getting': 415,\n",
       " 'yes': 416,\n",
       " 'become': 417,\n",
       " 'use': 418,\n",
       " 'loved': 419,\n",
       " 'human': 420,\n",
       " 'came': 421,\n",
       " 'absolutely': 422,\n",
       " 'others': 423,\n",
       " 'totally': 424,\n",
       " 'called': 425,\n",
       " 'lines': 426,\n",
       " 'fans': 427,\n",
       " 'certainly': 428,\n",
       " 'side': 429,\n",
       " \"they're\": 430,\n",
       " 'days': 431,\n",
       " 'doing': 432,\n",
       " 'entertaining': 433,\n",
       " 'example': 434,\n",
       " 'dark': 435,\n",
       " 'overall': 436,\n",
       " 'live': 437,\n",
       " 'art': 438,\n",
       " 'killer': 439,\n",
       " 'definitely': 440,\n",
       " 'lost': 441,\n",
       " 'went': 442,\n",
       " 'seemed': 443,\n",
       " 'city': 444,\n",
       " 'beginning': 445,\n",
       " 'friend': 446,\n",
       " 'tries': 447,\n",
       " 'problem': 448,\n",
       " 'town': 449,\n",
       " 'picture': 450,\n",
       " 'turn': 451,\n",
       " 'god': 452,\n",
       " 'remember': 453,\n",
       " 'against': 454,\n",
       " 'evil': 455,\n",
       " 'entire': 456,\n",
       " 'care': 457,\n",
       " 'wanted': 458,\n",
       " 'already': 459,\n",
       " 'fine': 460,\n",
       " 'sex': 461,\n",
       " 'actress': 462,\n",
       " 'close': 463,\n",
       " 'women': 464,\n",
       " 'itself': 465,\n",
       " 'particularly': 466,\n",
       " 'title': 467,\n",
       " 'history': 468,\n",
       " 'act': 469,\n",
       " 'throughout': 470,\n",
       " 'writing': 471,\n",
       " 'myself': 472,\n",
       " 'episode': 473,\n",
       " \"won't\": 474,\n",
       " 'white': 475,\n",
       " 'under': 476,\n",
       " 'cinema': 477,\n",
       " 'etc': 478,\n",
       " 'sound': 479,\n",
       " \"she's\": 480,\n",
       " 'unfortunately': 481,\n",
       " 'anyway': 482,\n",
       " 'oh': 483,\n",
       " 'wants': 484,\n",
       " 'hand': 485,\n",
       " 'direction': 486,\n",
       " 'piece': 487,\n",
       " 'cannot': 488,\n",
       " 'daughter': 489,\n",
       " 'stuff': 490,\n",
       " '5': 491,\n",
       " 'behind': 492,\n",
       " 'despite': 493,\n",
       " 'themselves': 494,\n",
       " 'humor': 495,\n",
       " 'lives': 496,\n",
       " 'children': 497,\n",
       " 'eyes': 498,\n",
       " 'lead': 499,\n",
       " 'highly': 500,\n",
       " 'able': 501,\n",
       " 'late': 502,\n",
       " 'moment': 503,\n",
       " 'hour': 504,\n",
       " 'person': 505,\n",
       " 'becomes': 506,\n",
       " 'kid': 507,\n",
       " 'waste': 508,\n",
       " 'child': 509,\n",
       " 'parts': 510,\n",
       " 'leave': 511,\n",
       " 'favorite': 512,\n",
       " 'viewer': 513,\n",
       " 'kill': 514,\n",
       " 'slow': 515,\n",
       " 'says': 516,\n",
       " 'thinking': 517,\n",
       " \"you'll\": 518,\n",
       " 'guys': 519,\n",
       " 'murder': 520,\n",
       " 'soon': 521,\n",
       " 'decent': 522,\n",
       " 'age': 523,\n",
       " 'past': 524,\n",
       " 'expect': 525,\n",
       " 'girls': 526,\n",
       " 'type': 527,\n",
       " 'guess': 528,\n",
       " 'voice': 529,\n",
       " '4': 530,\n",
       " 'happen': 531,\n",
       " 'brilliant': 532,\n",
       " 'b': 533,\n",
       " 'obvious': 534,\n",
       " 'matter': 535,\n",
       " 'several': 536,\n",
       " 'starts': 537,\n",
       " 'feeling': 538,\n",
       " 'michael': 539,\n",
       " 'heart': 540,\n",
       " 'final': 541,\n",
       " 'genre': 542,\n",
       " 'ok': 543,\n",
       " 'looked': 544,\n",
       " 'directed': 545,\n",
       " 'except': 546,\n",
       " 'involved': 547,\n",
       " 'flick': 548,\n",
       " 'police': 549,\n",
       " 'chance': 550,\n",
       " 'sometimes': 551,\n",
       " 'laugh': 552,\n",
       " 'blood': 553,\n",
       " 'opinion': 554,\n",
       " 'including': 555,\n",
       " 'experience': 556,\n",
       " 'number': 557,\n",
       " 'strong': 558,\n",
       " 'alone': 559,\n",
       " 'power': 560,\n",
       " 'gave': 561,\n",
       " 'robert': 562,\n",
       " 'attempt': 563,\n",
       " 'score': 564,\n",
       " 'works': 565,\n",
       " 'possible': 566,\n",
       " 'writer': 567,\n",
       " 'david': 568,\n",
       " 'told': 569,\n",
       " 'annoying': 570,\n",
       " 'somewhat': 571,\n",
       " 'known': 572,\n",
       " \"i'll\": 573,\n",
       " 'wonder': 574,\n",
       " 'stop': 575,\n",
       " 'cut': 576,\n",
       " \"aren't\": 577,\n",
       " 'episodes': 578,\n",
       " 'enjoyed': 579,\n",
       " 'killed': 580,\n",
       " 'took': 581,\n",
       " 'turned': 582,\n",
       " 'wish': 583,\n",
       " 'happens': 584,\n",
       " 'words': 585,\n",
       " 'turns': 586,\n",
       " \"film's\": 587,\n",
       " 'amazing': 588,\n",
       " 'english': 589,\n",
       " 'happened': 590,\n",
       " 'today': 591,\n",
       " 'lady': 592,\n",
       " 'view': 593,\n",
       " 'whose': 594,\n",
       " 'sequel': 595,\n",
       " 'horrible': 596,\n",
       " 'hit': 597,\n",
       " 'ago': 598,\n",
       " 'jane': 599,\n",
       " 'simple': 600,\n",
       " 'female': 601,\n",
       " 'quality': 602,\n",
       " 'scary': 603,\n",
       " 'coming': 604,\n",
       " 'seriously': 605,\n",
       " 'mystery': 606,\n",
       " 'save': 607,\n",
       " 'james': 608,\n",
       " 'relationship': 609,\n",
       " 'group': 610,\n",
       " 'please': 611,\n",
       " 'roles': 612,\n",
       " 'novel': 613,\n",
       " 'single': 614,\n",
       " 's': 615,\n",
       " 'middle': 616,\n",
       " 'musical': 617,\n",
       " 'hell': 618,\n",
       " 'across': 619,\n",
       " 'word': 620,\n",
       " \"wouldn't\": 621,\n",
       " 'level': 622,\n",
       " 'serious': 623,\n",
       " 'lack': 624,\n",
       " 'usually': 625,\n",
       " 'mostly': 626,\n",
       " 'shown': 627,\n",
       " 'eye': 628,\n",
       " 'king': 629,\n",
       " 'red': 630,\n",
       " 'sad': 631,\n",
       " 'crap': 632,\n",
       " 'bring': 633,\n",
       " 'york': 634,\n",
       " 'body': 635,\n",
       " 'running': 636,\n",
       " 'car': 637,\n",
       " 'jokes': 638,\n",
       " 'herself': 639,\n",
       " 'hours': 640,\n",
       " 'light': 641,\n",
       " 'self': 642,\n",
       " 'happy': 643,\n",
       " '7': 644,\n",
       " 'gore': 645,\n",
       " 'events': 646,\n",
       " 'obviously': 647,\n",
       " \"'\": 648,\n",
       " 'room': 649,\n",
       " 'basically': 650,\n",
       " 'british': 651,\n",
       " 'fight': 652,\n",
       " 'important': 653,\n",
       " 'miss': 654,\n",
       " 'comic': 655,\n",
       " 'cool': 656,\n",
       " 'son': 657,\n",
       " 'husband': 658,\n",
       " 'interest': 659,\n",
       " 'extremely': 660,\n",
       " 'documentary': 661,\n",
       " 'due': 662,\n",
       " 'local': 663,\n",
       " 'stories': 664,\n",
       " 'hilarious': 665,\n",
       " 'none': 666,\n",
       " 'predictable': 667,\n",
       " 'shots': 668,\n",
       " 'thriller': 669,\n",
       " 'started': 670,\n",
       " 'huge': 671,\n",
       " 'brother': 672,\n",
       " 'straight': 673,\n",
       " 'elements': 674,\n",
       " 'taken': 675,\n",
       " 'ten': 676,\n",
       " 'lame': 677,\n",
       " 'call': 678,\n",
       " 'dull': 679,\n",
       " 'oscar': 680,\n",
       " 'above': 681,\n",
       " 'yourself': 682,\n",
       " 'add': 683,\n",
       " 'richard': 684,\n",
       " 'near': 685,\n",
       " 'surprised': 686,\n",
       " 'rating': 687,\n",
       " 'released': 688,\n",
       " 'game': 689,\n",
       " 'clearly': 690,\n",
       " 'hero': 691,\n",
       " 'sets': 692,\n",
       " 'usual': 693,\n",
       " 'ridiculous': 694,\n",
       " 'storyline': 695,\n",
       " 'disappointed': 696,\n",
       " 'complete': 697,\n",
       " 'talent': 698,\n",
       " 'romantic': 699,\n",
       " 'entertainment': 700,\n",
       " 'country': 701,\n",
       " 'television': 702,\n",
       " \"let's\": 703,\n",
       " 'lee': 704,\n",
       " 'fast': 705,\n",
       " 'supporting': 706,\n",
       " 'violence': 707,\n",
       " 'similar': 708,\n",
       " 'dialog': 709,\n",
       " '9': 710,\n",
       " 'falls': 711,\n",
       " 'heard': 712,\n",
       " 'apparently': 713,\n",
       " 'finds': 714,\n",
       " 'japanese': 715,\n",
       " 'nor': 716,\n",
       " 'giving': 717,\n",
       " 'twist': 718,\n",
       " 'beyond': 719,\n",
       " 'attention': 720,\n",
       " 'ones': 721,\n",
       " 'whether': 722,\n",
       " 'editing': 723,\n",
       " 'songs': 724,\n",
       " 'tale': 725,\n",
       " 'song': 726,\n",
       " 'die': 727,\n",
       " \"haven't\": 728,\n",
       " 'minute': 729,\n",
       " 'fall': 730,\n",
       " 'peter': 731,\n",
       " 'lots': 732,\n",
       " 'exactly': 733,\n",
       " 'figure': 734,\n",
       " 'saying': 735,\n",
       " 'taking': 736,\n",
       " 'moving': 737,\n",
       " 'possibly': 738,\n",
       " 'reality': 739,\n",
       " 'screenplay': 740,\n",
       " 'strange': 741,\n",
       " 'wait': 742,\n",
       " 'non': 743,\n",
       " 'jack': 744,\n",
       " 'typical': 745,\n",
       " 'comments': 746,\n",
       " 'stay': 747,\n",
       " 'means': 748,\n",
       " 'future': 749,\n",
       " 'realistic': 750,\n",
       " 'cinematography': 751,\n",
       " 'mention': 752,\n",
       " 'sit': 753,\n",
       " 'somehow': 754,\n",
       " 'sorry': 755,\n",
       " 'named': 756,\n",
       " 'animation': 757,\n",
       " 'indeed': 758,\n",
       " 'message': 759,\n",
       " 'talk': 760,\n",
       " 'living': 761,\n",
       " 'street': 762,\n",
       " 'modern': 763,\n",
       " 'rock': 764,\n",
       " 'ends': 765,\n",
       " \"who's\": 766,\n",
       " 'class': 767,\n",
       " 'earth': 768,\n",
       " 'kept': 769,\n",
       " 'brought': 770,\n",
       " 'imagine': 771,\n",
       " 'major': 772,\n",
       " 'third': 773,\n",
       " 'nearly': 774,\n",
       " 'sequence': 775,\n",
       " 'atmosphere': 776,\n",
       " 'bunch': 777,\n",
       " 'theme': 778,\n",
       " 'reviews': 779,\n",
       " 'christmas': 780,\n",
       " 'whom': 781,\n",
       " 'expected': 782,\n",
       " 'fantastic': 783,\n",
       " 'hate': 784,\n",
       " 'opening': 785,\n",
       " 'easily': 786,\n",
       " 'check': 787,\n",
       " 'imdb': 788,\n",
       " 'doubt': 789,\n",
       " 'enjoyable': 790,\n",
       " 'tells': 791,\n",
       " 'baby': 792,\n",
       " 'appears': 793,\n",
       " 'sounds': 794,\n",
       " 'career': 795,\n",
       " 'laughs': 796,\n",
       " 'writers': 797,\n",
       " 'air': 798,\n",
       " 'nature': 799,\n",
       " 're': 800,\n",
       " 'cheap': 801,\n",
       " 'dream': 802,\n",
       " 'sadly': 803,\n",
       " 'easy': 804,\n",
       " 'emotional': 805,\n",
       " 'within': 806,\n",
       " 'talking': 807,\n",
       " 't': 808,\n",
       " 'ways': 809,\n",
       " 'knew': 810,\n",
       " 'begins': 811,\n",
       " 'four': 812,\n",
       " 'five': 813,\n",
       " 'effort': 814,\n",
       " 'viewers': 815,\n",
       " 'silly': 816,\n",
       " 'material': 817,\n",
       " 'comment': 818,\n",
       " 'knows': 819,\n",
       " 'meet': 820,\n",
       " 'stand': 821,\n",
       " 'parents': 822,\n",
       " 'difficult': 823,\n",
       " 'hot': 824,\n",
       " 'upon': 825,\n",
       " 'leads': 826,\n",
       " 'avoid': 827,\n",
       " 'points': 828,\n",
       " 'suspense': 829,\n",
       " 'french': 830,\n",
       " 'gone': 831,\n",
       " 'believable': 832,\n",
       " 'weird': 833,\n",
       " 'wasted': 834,\n",
       " 'review': 835,\n",
       " 'period': 836,\n",
       " 'dramatic': 837,\n",
       " 'season': 838,\n",
       " 'leading': 839,\n",
       " 'wrote': 840,\n",
       " 'books': 841,\n",
       " 'george': 842,\n",
       " 'order': 843,\n",
       " 'directors': 844,\n",
       " \"what's\": 845,\n",
       " 'directing': 846,\n",
       " 'hear': 847,\n",
       " 'portrayed': 848,\n",
       " 'famous': 849,\n",
       " 'using': 850,\n",
       " 'casting': 851,\n",
       " '8': 852,\n",
       " 'release': 853,\n",
       " 'working': 854,\n",
       " 'change': 855,\n",
       " 'problems': 856,\n",
       " 'among': 857,\n",
       " 'feels': 858,\n",
       " 'theater': 859,\n",
       " 'tried': 860,\n",
       " 'appear': 861,\n",
       " 'setting': 862,\n",
       " 'needs': 863,\n",
       " 'married': 864,\n",
       " 'deal': 865,\n",
       " 'sequences': 866,\n",
       " 'decided': 867,\n",
       " 'perfectly': 868,\n",
       " 'begin': 869,\n",
       " 'hair': 870,\n",
       " 'clear': 871,\n",
       " 'interested': 872,\n",
       " \"you've\": 873,\n",
       " 'space': 874,\n",
       " 'return': 875,\n",
       " 'poorly': 876,\n",
       " 'depth': 877,\n",
       " 'western': 878,\n",
       " 'box': 879,\n",
       " 'odd': 880,\n",
       " 'follow': 881,\n",
       " 'social': 882,\n",
       " 'girlfriend': 883,\n",
       " 'meant': 884,\n",
       " 'particular': 885,\n",
       " 'premise': 886,\n",
       " 'otherwise': 887,\n",
       " 'forced': 888,\n",
       " 'shame': 889,\n",
       " 'potential': 890,\n",
       " 'de': 891,\n",
       " 'worked': 892,\n",
       " 'eventually': 893,\n",
       " 'meets': 894,\n",
       " 'sister': 895,\n",
       " 'stage': 896,\n",
       " 'fantasy': 897,\n",
       " 'animals': 898,\n",
       " 'became': 899,\n",
       " 'previous': 900,\n",
       " 'needed': 901,\n",
       " 'whatever': 902,\n",
       " 'earlier': 903,\n",
       " 'stewart': 904,\n",
       " 'waiting': 905,\n",
       " 'brings': 906,\n",
       " 'cute': 907,\n",
       " 'agree': 908,\n",
       " 'failed': 909,\n",
       " 'showing': 910,\n",
       " 'result': 911,\n",
       " 'effect': 912,\n",
       " 'bought': 913,\n",
       " 'gay': 914,\n",
       " 'badly': 915,\n",
       " 'note': 916,\n",
       " 'creepy': 917,\n",
       " 'fire': 918,\n",
       " 'paul': 919,\n",
       " 'apart': 920,\n",
       " 'move': 921,\n",
       " 'mary': 922,\n",
       " 'general': 923,\n",
       " 'form': 924,\n",
       " 'average': 925,\n",
       " 'doctor': 926,\n",
       " 'co': 927,\n",
       " 'pathetic': 928,\n",
       " 'admit': 929,\n",
       " 'sci': 930,\n",
       " 'fi': 931,\n",
       " 'feature': 932,\n",
       " 'frank': 933,\n",
       " 'honestly': 934,\n",
       " 'rent': 935,\n",
       " 'ask': 936,\n",
       " 'greatest': 937,\n",
       " 'band': 938,\n",
       " 'unless': 939,\n",
       " 'crime': 940,\n",
       " 'bored': 941,\n",
       " 'society': 942,\n",
       " 'hands': 943,\n",
       " 'beauty': 944,\n",
       " 'basic': 945,\n",
       " 'kelly': 946,\n",
       " 'fighting': 947,\n",
       " 'features': 948,\n",
       " 'dance': 949,\n",
       " \"we're\": 950,\n",
       " 'filmed': 951,\n",
       " 'hardly': 952,\n",
       " '20': 953,\n",
       " 'romance': 954,\n",
       " 'members': 955,\n",
       " 'cartoon': 956,\n",
       " 'older': 957,\n",
       " 'male': 958,\n",
       " 'weak': 959,\n",
       " 'copy': 960,\n",
       " 'ben': 961,\n",
       " 'free': 962,\n",
       " 'cop': 963,\n",
       " 'science': 964,\n",
       " 'certain': 965,\n",
       " 'monster': 966,\n",
       " 'write': 967,\n",
       " 'buy': 968,\n",
       " 'expecting': 969,\n",
       " 'situation': 970,\n",
       " 'surprise': 971,\n",
       " 'cat': 972,\n",
       " 'team': 973,\n",
       " 'consider': 974,\n",
       " 'state': 975,\n",
       " 'soundtrack': 976,\n",
       " 'credits': 977,\n",
       " 'actual': 978,\n",
       " 'footage': 979,\n",
       " 'break': 980,\n",
       " 'trash': 981,\n",
       " 'joe': 982,\n",
       " 'tom': 983,\n",
       " 'cheesy': 984,\n",
       " 'crazy': 985,\n",
       " 'front': 986,\n",
       " 'control': 987,\n",
       " 'business': 988,\n",
       " 'deserves': 989,\n",
       " 'decide': 990,\n",
       " 'sexual': 991,\n",
       " 'dumb': 992,\n",
       " 'present': 993,\n",
       " 'forward': 994,\n",
       " 'zombie': 995,\n",
       " 'sees': 996,\n",
       " 'joke': 997,\n",
       " 'attempts': 998,\n",
       " 'recently': 999,\n",
       " '15': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 13,\n",
       " 33,\n",
       " 422,\n",
       " 402,\n",
       " 18,\n",
       " 91,\n",
       " 27,\n",
       " 6893,\n",
       " 8,\n",
       " 31,\n",
       " 1311,\n",
       " 4504,\n",
       " 39,\n",
       " 539,\n",
       " 8007,\n",
       " 194,\n",
       " 24,\n",
       " 75,\n",
       " 161,\n",
       " 17,\n",
       " 11,\n",
       " 228,\n",
       " 336,\n",
       " 27,\n",
       " 68,\n",
       " 252,\n",
       " 203,\n",
       " 8,\n",
       " 468,\n",
       " 58,\n",
       " 68,\n",
       " 75,\n",
       " 108,\n",
       " 98,\n",
       " 21,\n",
       " 4165,\n",
       " 11,\n",
       " 1418,\n",
       " 694,\n",
       " 695,\n",
       " 11,\n",
       " 18,\n",
       " 6,\n",
       " 33,\n",
       " 389,\n",
       " 6065,\n",
       " 204,\n",
       " 2345,\n",
       " 487,\n",
       " 1,\n",
       " 86,\n",
       " 928,\n",
       " 140,\n",
       " 69,\n",
       " 158,\n",
       " 51,\n",
       " 1,\n",
       " 16730,\n",
       " 8008,\n",
       " 69,\n",
       " 251,\n",
       " 68,\n",
       " 2895,\n",
       " 15,\n",
       " 16731,\n",
       " 1877,\n",
       " 12117,\n",
       " 12118,\n",
       " 1344,\n",
       " 4505,\n",
       " 3,\n",
       " 38,\n",
       " 3640,\n",
       " 120,\n",
       " 1386,\n",
       " 16,\n",
       " 4504,\n",
       " 13,\n",
       " 164,\n",
       " 17,\n",
       " 2,\n",
       " 928,\n",
       " 805,\n",
       " 8009,\n",
       " 8,\n",
       " 2,\n",
       " 18,\n",
       " 12,\n",
       " 13,\n",
       " 3231,\n",
       " 4,\n",
       " 99,\n",
       " 142,\n",
       " 1239,\n",
       " 9,\n",
       " 221,\n",
       " 696,\n",
       " 12,\n",
       " 48,\n",
       " 24,\n",
       " 97,\n",
       " 35,\n",
       " 11,\n",
       " 8010,\n",
       " 4506,\n",
       " 35,\n",
       " 1311,\n",
       " 16732,\n",
       " 49,\n",
       " 375,\n",
       " 9,\n",
       " 98,\n",
       " 1006,\n",
       " 753,\n",
       " 137,\n",
       " 10]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary that maps each word to a token can be accessed with `tokenizer.word_index`\n",
    "    \n",
    "❓ **Question** ❓ Add a `vocab_size` variable that stores the number of different words (=tokens) in the train set. This is called the _size of the vocabulary_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30419"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = Tokenizer()\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `X_train_token` and `X_test_token` contain sequences of different lengths.\n",
    "\n",
    "<img src=\"padding.png\" alt='Word2Vec' width=\"700px\" />\n",
    "\n",
    "However, a neural network has to have a tensor as input. For this reason, you have to pad your data.\n",
    "\n",
    "❓ **Question** ❓  Pad your data with the `pad_sequences` function (documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)). Do not forget about the `dtype` and `padding` keywords (but do not use `maxlen` here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 1164)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Pad the inputs\n",
    "X_pad = pad_sequences(X_train_token, dtype='float32', padding='post', value=0)\n",
    "X_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30419"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The RNN\n",
    "\n",
    "Let's now feed this data to a Recurrent Neural Network.\n",
    "\n",
    "❓ **Question** ❓ Write a model that has:\n",
    "- an embedding layer whose `input_dim` is the size of your vocabulary (= your `vocab_size`), and whose `output_dim` is the size of the embedding space you want to have\n",
    "- a RNN (SimpleRNN, LSTM, GRU) layer\n",
    "- a Dense layer\n",
    "- an output layer\n",
    "\n",
    "⚠️ **Warning** ⚠️ Here, you don't need a masking layer. Why? Because `layers.Embedding` has a argument to do that directly, which you have to set with `mask_zero=True`. That also means that your data **HAS TO** be padded with **0** (which is the default behavior). See the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding#example_2) to understand how it **impacts** the `input_dim`.\n",
    "\n",
    "<details>\n",
    "    <summary>💡 Hint</summary>\n",
    "\n",
    "`input_dim` should equal size of vocabulary + 1\n",
    "\n",
    "</details>\n",
    "\n",
    "Compile it with the appropriate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "### Let's build the neural network now\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(patience=4, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# RNN\n",
    "rnn = Sequential([\n",
    "    layers.Embedding(input_dim=30419+1, output_dim=30, input_length=1164, mask_zero=True),\n",
    "    layers.LSTM(units=10, activation='tanh'),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "rnn.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 26s 360ms/step - loss: 0.6906 - accuracy: 0.5435 - val_loss: 0.6839 - val_accuracy: 0.6400\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 22s 342ms/step - loss: 0.6265 - accuracy: 0.7435 - val_loss: 0.5523 - val_accuracy: 0.7620\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 21s 341ms/step - loss: 0.4385 - accuracy: 0.8590 - val_loss: 0.4801 - val_accuracy: 0.8000\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 22s 343ms/step - loss: 0.3129 - accuracy: 0.9165 - val_loss: 0.4818 - val_accuracy: 0.7900\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 22s 348ms/step - loss: 0.2242 - accuracy: 0.9465 - val_loss: 0.4199 - val_accuracy: 0.8120\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 22s 351ms/step - loss: 0.1545 - accuracy: 0.9655 - val_loss: 0.4616 - val_accuracy: 0.8060\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 22s 350ms/step - loss: 0.1109 - accuracy: 0.9760 - val_loss: 0.4750 - val_accuracy: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x148070250>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_pad,y_train, epochs=10, batch_size=32, verbose=1, callbacks=es, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Look at the number of parameters in your RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 1164, 30)          912600    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 10)                1640      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 914,251\n",
      "Trainable params: 914,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "912600"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*(+1+30419)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Double-check that the number of parameters in your embedding layer is equal to the (number of words in your vocabulary + 1 for the masking value) $\\times$  the dimension of your embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 13s 207ms/step - loss: 0.1547 - accuracy: 0.9675 - val_loss: 0.4027 - val_accuracy: 0.8220\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 13s 206ms/step - loss: 0.1019 - accuracy: 0.9820 - val_loss: 0.5751 - val_accuracy: 0.7920\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 13s 205ms/step - loss: 0.0800 - accuracy: 0.9825 - val_loss: 0.4894 - val_accuracy: 0.8120\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 13s 201ms/step - loss: 0.0515 - accuracy: 0.9915 - val_loss: 0.5269 - val_accuracy: 0.8080\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 13s 203ms/step - loss: 0.0355 - accuracy: 0.9955 - val_loss: 0.6033 - val_accuracy: 0.8020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2982cdb70>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_pad,y_train, epochs=20, batch_size=32, verbose=1, callbacks=es, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Start fitting your model with 20 epochs, with an early stopping criterion whose patience is equal to 4.\n",
    "\n",
    "⚠️ **Warning** ⚠️ You might see that it takes a lot of time! \n",
    "\n",
    "**So stop it after a couple of iterations!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not waste too much time just staring at our screen or having coffees. It is too early to start having breaks ;)\n",
    "\n",
    "❓ **Question** ❓ We will reduce the computational time. To start, let's first look at how many words there are in the different sentences of your train set (Just run the following cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_hist(X):\n",
    "    len_ = [len(_) for _ in X]\n",
    "    plt.hist(len_)\n",
    "    plt.title('Histogram of the number of sentences that have a given number of words')\n",
    "    plt.show()\n",
    "    \n",
    "plot_hist(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably see that 90 to 95% of your sentences have less than 300 words. And very few have more than 1000.\n",
    "\n",
    "However, as you didn't use `maxlen` in your padding above, your input tensor has a dimension equal to the length of the sentence that has the maximum number of words.\n",
    "\n",
    "Now, let's look at how this affects the padding: \n",
    "\n",
    "\n",
    "<img src=\"tensor_size.png\" alt='Word2Vec' width=\"700px\" />\n",
    "\n",
    "Because of a few of very long sentences, one dimension of your tensor is equal to around 1000. However, most of the sentences with ~200 words have just padded values that are useless.\n",
    "\n",
    "So your tensor is mostly useless information, which still adds time to the training process.\n",
    "\n",
    "But what if you pad the data to a maximum length (`maxlen`) of say 200 (words)?\n",
    "- First, that would increase the convergence and you would not need to stare at your screen while waiting for the algorithm to converge\n",
    "- But in essence, do you really lose that much information? Do you think that you often need more than 200 words (up to 1000) to tell whether or not a sentence is positive of negative?\n",
    "\n",
    "❓ **Question** ❓ For all these reasons, re-do your padding using the `maxlen` keyword and retrain the model!  See how much faster it is now - without hurting the performance ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 200)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Pad the inputs\n",
    "X_pad = pad_sequences(X_train_token, dtype='float32', padding='post', value=0, maxlen=200)\n",
    "X_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "rnn = Sequential([\n",
    "    layers.Embedding(input_dim=30419+1, output_dim=30, input_length=1164, mask_zero=True),\n",
    "    layers.LSTM(units=10, activation='tanh'),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "rnn.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 1164), found shape=(None, 200)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/11/qmf8fycd0_d7cflbxhz7fnfw0000gn/T/__autograph_generated_filee3f_87n1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/nico/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 1164), found shape=(None, 200)\n"
     ]
    }
   ],
   "source": [
    "rnn.fit(X_pad,y_train, epochs=10, batch_size=32, verbose=1, callbacks=es, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
